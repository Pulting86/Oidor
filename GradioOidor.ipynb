{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "vd_jazVPvXlO"
   },
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "import math\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "from docx import Document\n",
    "import torch\n",
    "import requests\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nadth4L4vbKW"
   },
   "outputs": [],
   "source": [
    "def split_audio(input_file, output_folder, duration):\n",
    "    audio = AudioSegment.from_mp3(input_file)\n",
    "    total_length = len(audio)\n",
    "    num_parts = math.ceil(total_length / (duration * 1000))\n",
    "\n",
    "    for i in range(num_parts):\n",
    "        start = i * duration * 1000\n",
    "        end = (i + 1) * duration * 1000\n",
    "        split_audio = audio[start:end]\n",
    "        output_path = os.path.join(output_folder, f\"part_{i+1}.mp3\")\n",
    "        split_audio.export(output_path, format=\"mp3\")\n",
    "        print(f\"Exported {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from docx import Document\n",
    "\n",
    "def censurar_docx(file_path, output_path):\n",
    "    \n",
    "    doc = Document(file_path)\n",
    "    \n",
    "    def censurar_texto(texto):\n",
    "        try:\n",
    "            messages = [{\"role\": \"user\", \"content\" : \"Substitute the obscene words in this text with ****, in case that there are no obscene words, return the document as it is. Answer only with the censored text,don't add anything else. And this is the text: \" + texto}]\n",
    "            response = ollama.chat(model=\"llama3.2\", messages=messages)\n",
    "            return response[\"message\"][\"content\"]\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error al conectarse a Ollama: {e}\")\n",
    "            return texto\n",
    "\n",
    "    for paragraph in doc.paragraphs:\n",
    "        original_text = paragraph.text\n",
    "        if original_text.strip():\n",
    "            censored_text = censurar_texto(original_text)\n",
    "            paragraph.text = censored_text\n",
    "    \n",
    "    doc.save(output_path)\n",
    "    print(f\"Documento censurado guardado en: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1, 2, 3! 1, 2, 3! It's a very good way to, very good way to start. 1, 2, 3! It's a very good way, it's a very good way to start. 4, 5, 6! It's a very good way to, very good way to start. 4, 5, 6! It's a very good way to, very good way to start. 1, 2, 3! 4, 5, 6! It's a very good way to start. 1, 2, 3, 4, 5, 6, it's a very good way to start These are the numbers we all know We'll learn more as we grow the numbers It wouldn't be even if we didn't have 7 It's a very good place to start It wouldn't be even if we didn't have 7 It's a very good place to start 7 and 8 and 9 and 10 It's a very good place to start 7 and 8 and 9 and 10 It's a very good place to start 1, 2, 3, 4, 5, 6 It's a very good place to start 7 and 8 and 9 and 10 It's a very good place to start These are the numbers we all know We'll learn more as we grow the numbers These are the numbers we all know Berlin, Borg, A.G.O. and Nobles fuck you bitch\n",
      "Documento censurado guardado en: censurado.docx\n"
     ]
    }
   ],
   "source": [
    "censurar_docx(\"transcripcion1.docx\", \"censurado.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribir_con_censura(file):\n",
    "  device = 0 if torch.cuda.is_available() else -1\n",
    "  pipe = pipeline(\"automatic-speech-recognition\", \"openai/whisper-large-v3\", return_timestamps=True, device=device)\n",
    "  output_dir = \"Salida\"\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "  split_audio(file, output_dir, 900)\n",
    "  doc = Document()\n",
    "\n",
    "  for file in sorted(os.listdir(output_dir)):\n",
    "      file_path = os.path.join(output_dir, file)\n",
    "      print(f\"Procesando: {file_path}\")\n",
    "\n",
    "      transcription = pipe(file_path)[\"text\"]\n",
    "      doc.add_paragraph(transcription)\n",
    "\n",
    "  output_file = \"transcripcion.docx\"\n",
    "  doc.save(output_file)\n",
    "\n",
    "  doc1 = \"censurado.docx\"\n",
    "\n",
    "  censurar_docx(output_file, doc1)\n",
    "\n",
    "  return doc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "9TvoB_sZv-Jx"
   },
   "outputs": [],
   "source": [
    "def transcribir_sin_censura(file):\n",
    "  device = 0 if torch.cuda.is_available() else -1\n",
    "  pipe = pipeline(\"automatic-speech-recognition\", \"openai/whisper-large-v3\", return_timestamps=True, device=device)\n",
    "  output_dir = \"Salida\"\n",
    "  os.makedirs(output_dir, exist_ok=True)\n",
    "  split_audio(file, output_dir, 900)\n",
    "  doc = Document()\n",
    "\n",
    "  for file in sorted(os.listdir(output_dir)):\n",
    "      file_path = os.path.join(output_dir, file)\n",
    "      print(f\"Procesando: {file_path}\")\n",
    "\n",
    "      transcription = pipe(file_path)[\"text\"]\n",
    "      doc.add_paragraph(transcription)\n",
    "\n",
    "  output_file = \"transcripcion.docx\"\n",
    "  doc.save(output_file)\n",
    "\n",
    "  return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Salida\\part_1.mp3\n",
      "Procesando: Salida\\part_1.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OscarZ\\Desktop\\Todo\\Estudio\\Oidor\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Blocks()\n",
    "\n",
    "with demo:\n",
    "    audio_file = gr.Audio(type=\"filepath\")\n",
    "\n",
    "    b1 = gr.Button(\"Con censura\")\n",
    "    b2 = gr.Button(\"Sin censura\")\n",
    "\n",
    "    b1.click(transcribir_con_censura, inputs=audio_file, outputs=gr.File())\n",
    "    b2.click(transcribir_sin_censura, inputs=audio_file, outputs=gr.File())\n",
    "\n",
    "demo.queue(default_concurrency_limit=10).launch(share=True,debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "RO0VYbmHwY4N",
    "outputId": "882ae009-e601-4139-86f6-d436fa4def39"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OscarZ\\Desktop\\Todo\\Estudio\\Oidor\\.venv\\Lib\\site-packages\\gradio\\interface.py:399: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Salida\\part_1.mp3\n",
      "Procesando: Salida\\part_1.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\OscarZ\\Desktop\\Todo\\Estudio\\Oidor\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento censurado guardado en: censurado.docx\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=transcribir_con_censura,\n",
    "    inputs=gr.File(),\n",
    "    outputs=gr.File(),\n",
    "    allow_flagging='never'\n",
    ")\n",
    "demo.launch(share=True,debug=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
